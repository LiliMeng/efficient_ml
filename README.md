# efficient_ml

[The KV Cache: Memory Usage in Transformers]
(https://www.youtube.com/watch?v=80bIUggRJf4&t=324s)

[Variants of Multi-head attention: Multi-query (MQA) and Grouped-query attention (GQA)]
(https://www.youtube.com/watch?v=pVP0bu8QA2w)
