# efficient_ml
1. The KV Cache: Memory Usage in Transformers [Video](https://www.youtube.com/watch?v=80bIUggRJf4&t=324s)

2. Variants of Multi-head attention: Multi-query (MQA) and Grouped-query attention (GQA) [Video](https://www.youtube.com/watch?v=pVP0bu8QA2w)

Fast LLM Serving with vLLM and PagedAttention [Video](https://www.youtube.com/watch?v=5ZlavKF_98U)
